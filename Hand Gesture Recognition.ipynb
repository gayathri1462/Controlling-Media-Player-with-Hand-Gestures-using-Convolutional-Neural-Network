{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition using Deep Learning\n",
    "\n",
    "\n",
    "## Team - 25\n",
    "\n",
    "## 1.Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data/train\")\n",
    "    os.makedirs(\"data/train/01_palm\")\n",
    "    os.makedirs(\"data/train/02_fist\")\n",
    "    os.makedirs(\"data/train/03_thumbs-up\")\n",
    "    os.makedirs(\"data/train/04_thumbs-down\")\n",
    "    os.makedirs(\"data/train/05_index-right\")\n",
    "    os.makedirs(\"data/train/06_index-left\")\n",
    "    os.makedirs(\"data/train/07_no-gesture\")\n",
    "    os.makedirs(\"data/test\")\n",
    "    os.makedirs(\"data/test/01_palm\")\n",
    "    os.makedirs(\"data/test/02_fist\")\n",
    "    os.makedirs(\"data/test/03_thumbs-up\")\n",
    "    os.makedirs(\"data/test/04_thumbs-down\")\n",
    "    os.makedirs(\"data/test/05_index-right\")\n",
    "    os.makedirs(\"data/test/06_index-left\")\n",
    "    os.makedirs(\"data/test/07_no-gesture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01_palm',\n",
       " '02_fist',\n",
       " '03_thumbs-up',\n",
       " '04_thumbs-down',\n",
       " '05_index-right',\n",
       " '06_index-left',\n",
       " '07_no-gesture']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01_palm',\n",
       " '02_fist',\n",
       " '03_thumbs-up',\n",
       " '04_thumbs-down',\n",
       " '05_index-right',\n",
       " '06_index-left',\n",
       " '07_no-gesture']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Mode - Train or Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter mode:\n"
     ]
    }
   ],
   "source": [
    "print('Enter mode:')\n",
    "mode = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'train':   \n",
    "    directory = 'data/train/'\n",
    "else:\n",
    "    directory = 'data/test/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture images for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    # Simulating mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Getting count of existing images\n",
    "    count = {'palm': len(os.listdir(directory+\"01_palm\")),\n",
    "             'fist': len(os.listdir(directory+\"02_fist\")),\n",
    "             'thumbs-up': len(os.listdir(directory+\"03_thumbs-up\")),\n",
    "             'thumbs-down': len(os.listdir(directory+\"04_thumbs-down\")),\n",
    "             'index-right': len(os.listdir(directory+\"05_index-right\")),\n",
    "             'index-left': len(os.listdir(directory+\"06_index-left\")),\n",
    "             'no-gesture': len(os.listdir(directory+\"07_no-gesture\")),\n",
    "            }\n",
    "\n",
    "    # Printing the count in each set to the screen\n",
    "    cv2.putText(frame, \"MODE: \"+mode, (10, 50),cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"IMAGE COUNT:\", (10, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"Raised Hand(0):\"+str(count['palm']), (10, 150), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"Raised Fist(1):\"+str(count['fist']), (10, 200), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"Thumbs-Up(2):\"+str(count['thumbs-up']), (10, 250), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"Thumbs-Down(3):\"+str(count['thumbs-down']), (10, 300), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"Index Pointing Right (4):\"+str(count['index-right']), (10, 350), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"Index Pointing Left(5):\"+str(count['index-left']), (10, 400), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "    cv2.putText(frame, \"No gesture(6):\"+str(count['no-gesture']), (10, 450), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "\n",
    "    x1 = int(0.5*frame.shape[1])\n",
    "    y1 = 10\n",
    "    x2 = frame.shape[1]-10\n",
    "    y2 = int(0.5*frame.shape[1])\n",
    "    # Drawing the ROI\n",
    "    # The increment/decrement by 1 is to compensate for the bounding box\n",
    "    cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0),3)\n",
    "    # Extracting the ROI\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    roi = cv2.resize(roi, (120, 120))\n",
    "\n",
    "    cv2.imshow(\"Collecting data\", frame)\n",
    "\n",
    "    #_, mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)\n",
    "    #kernel = np.ones((1, 1), np.uint8)\n",
    "    #img = cv2.dilate(mask, kernel, iterations=1)\n",
    "    #img = cv2.erode(mask, kernel, iterations=1)\n",
    "    # do the processing after capturing the image!\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, roi = cv2.threshold(roi, 180, 255, cv2.THRESH_BINARY)\n",
    "    cv2.imshow(\"ROI\", roi)\n",
    "\n",
    "    interrupt = cv2.waitKey(10)\n",
    "    if interrupt & 0xFF == 27: # esc key\n",
    "        break\n",
    "    if interrupt & 0xFF == ord('0'):\n",
    "        cv2.imwrite(directory+'01_palm/'+str(count['palm'])+'.jpg', roi)\n",
    "    if interrupt & 0xFF == ord('1'):\n",
    "        cv2.imwrite(directory+'02_fist/'+str(count['fist'])+'.jpg', roi)\n",
    "    if interrupt & 0xFF == ord('2'):\n",
    "        cv2.imwrite(directory+'03_thumbs-up/'+str(count['thumbs-up'])+'.jpg', roi)\n",
    "    if interrupt & 0xFF == ord('3'):\n",
    "        cv2.imwrite(directory+'04_thumbs-down/'+str(count['thumbs-down'])+'.jpg', roi)\n",
    "    if interrupt & 0xFF == ord('4'):\n",
    "        cv2.imwrite(directory+'05_index-right/'+str(count['index-right'])+'.jpg', roi)\n",
    "    if interrupt & 0xFF == ord('5'):\n",
    "        cv2.imwrite(directory+'06_index-left/'+str(count['index-left'])+'.jpg', roi)\n",
    "    if interrupt & 0xFF == ord('6'):\n",
    "        cv2.imwrite(directory+'07_no-gesture/'+str(count['no-gesture'])+'.jpg', roi)\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestures in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\01_palm\\\\0.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\02_fist\\\\0.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\03_thumbs-up\\\\4.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\04_thumbs-down\\\\15.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\05_index-right\\\\15.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\06_index-left\\\\7.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('D:\\\\6TH SEM\\\\MP\\data\\\\train\\\\07_no-gesture\\\\15.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Training CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of keras model and hidden layers for our convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 120, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Second convolution layer and pooling\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Third convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# Flattening the layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
    "              loss='categorical_crossentropy', # Loss function, which tells us how bad our predictions are.\n",
    "              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the train/test data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('data/train',\n",
    "                                                 target_size=(120, 120),\n",
    "                                                 batch_size=7,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('data/test',\n",
    "                                            target_size=(120, 120),\n",
    "                                            batch_size=7,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        training_set,\n",
    "        steps_per_epoch=125, # No of images in training set\n",
    "        epochs=7,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=50)# No of images in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model.evaluate(training_set)\n",
    "\n",
    "print('Train accuracy: {:2.2f}%'.format(train_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_set)\n",
    "\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save entire model to a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('handrecognition_model.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"gesture-model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('gesture-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hand Gesture Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import operator\n",
    "import cv2\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"gesture-model.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"gesture-model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Category dictionary\n",
    "categories = {0: 'palm', 1: 'fist', 2: 'thumbs-up', 3: 'thumbs-down', 4: 'index-right', 5: 'index-left', 6:'no-gesture'}\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    # Simulating mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Got this from collect-data.py\n",
    "    # Coordinates of the ROI\n",
    "    x1 = int(0.5*frame.shape[1])\n",
    "    y1 = 10\n",
    "    x2 = frame.shape[1]-10\n",
    "    y2 = int(0.5*frame.shape[1])\n",
    "\n",
    "\n",
    "    # Drawing the ROI\n",
    "    # The increment/decrement by 1 is to compensate for the bounding box\n",
    "    cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0),3)\n",
    "    # Extracting the ROI\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    # Resizing the ROI so it can be fed to the model for prediction\n",
    "    roi = cv2.resize(roi, (120, 120))\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, test_image = cv2.threshold(roi, 180, 255, cv2.THRESH_BINARY)\n",
    "    cv2.imshow(\"Test Image\", test_image)\n",
    "    # Batch of 1\n",
    "    result = loaded_model.predict(test_image.reshape(1, 120, 120, 1))\n",
    "    prediction = {'palm': result[0][0],\n",
    "                  'fist': result[0][1],\n",
    "                  'thumbs-up': result[0][2],\n",
    "                  'thumbs-down': result[0][3],\n",
    "                  'index-right': result[0][4],\n",
    "                  'index-left': result[0][5],\n",
    "                  'no-gesture':result[0][6]}\n",
    "    # Sorting based on top prediction\n",
    "    prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    # Displaying the predictions\n",
    "    cv2.putText(frame, prediction[0][0], (10, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,0,0), 3)\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    interrupt = cv2.waitKey(10)\n",
    "    if interrupt & 0xFF == 27: # esc key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Hand Gestures to Control Media Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label = \"\"\n",
    "action=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "while (vid.isOpened()):\n",
    "\n",
    "    ret,frame = vid.read()\n",
    "    if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Got this from collect-data.py\n",
    "            # Coordinates of the ROI\n",
    "            x1 = int(0.5*frame.shape[1])\n",
    "            y1 = 10\n",
    "            x2 = frame.shape[1]-10\n",
    "            y2 = int(0.5*frame.shape[1])\n",
    "            # Drawing the ROI\n",
    "            # The increment/decrement by 1 is to compensate for the bounding box\n",
    "            cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0),3)\n",
    "            # Extracting the ROI\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Resizing the ROI so it can be fed to the model for prediction\n",
    "            roi = cv2.resize(roi, (120, 120))\n",
    "            roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            _, test_image = cv2.threshold(roi, 180, 255, cv2.THRESH_BINARY)\n",
    "            cv2.imshow(\"Test Image\", test_image)\n",
    "            result = loaded_model.predict(test_image.reshape(1, 120, 120, 1))\n",
    "            prediction = {'palm': result[0][0],\n",
    "                          'fist': result[0][1],\n",
    "                          'thumbs-up': result[0][2],\n",
    "                          'thumbs-down': result[0][3],\n",
    "                          'index-right': result[0][4],\n",
    "                          'index-left': result[0][5],\n",
    "                          'no-gesture':result[0][6]}\n",
    "                # Sorting based on top prediction\n",
    "            prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "            if(prediction[0][0] == 'palm'):\n",
    "                final_label = 'palm'\n",
    "                action = \"PLAY/PAUSE\"\n",
    "                pyautogui.press('playpause', presses=1)\n",
    "            elif (prediction[0][0] == 'fist'):\n",
    "                final_label = 'fist'\n",
    "                action = \"MUTE\"\n",
    "                pyautogui.press('volumemute', presses=1)\n",
    "            elif (prediction[0][0] == 'thumbs-up'):\n",
    "                final_label = \"thumbs-up\"\n",
    "                action = \"VOLUME UP\"\n",
    "                pyautogui.press('volumeup', presses=1)\n",
    "            elif (prediction[0][0] == \"thumbs-down\"):\n",
    "                final_label = \"thumbs-down\"\n",
    "                action = \"VOLUME DOWN\"\n",
    "                pyautogui.press('volumedown', presses=1)\n",
    "            elif (prediction[0][0] == \"index-right\"):\n",
    "                final_label = \"index-right\"\n",
    "                action = \"FORWARD\"\n",
    "                pyautogui.press('nexttrack', presses=1)\n",
    "            elif (prediction[0][0] == \"index-left\"):\n",
    "                final_label = \"index-left\"\n",
    "                action = \"REWIND\"\n",
    "                pyautogui.press('prevtrack', presses=1)\n",
    "            elif (prediction[0][0] == \"no-gesture\"):\n",
    "                final_label = \"no-gesture\"\n",
    "                action = \"NO-ACTION\"\n",
    "            text1= \"Gesture: {}\".format(final_label)\n",
    "            text2= \"Action:{}\".format(action)\n",
    "\n",
    "            cv2.putText(frame, text1 , (10, 120), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "            cv2.putText(frame, text2 , (10, 220), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,0,0), 1)\n",
    "            cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
